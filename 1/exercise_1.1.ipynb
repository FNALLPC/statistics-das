{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File: exercise_1.1.py\n",
    "## Analyze the following Run I CMS $H \\rightarrow 4 \\ell$ data\n",
    "\n",
    " - Created: 18-Dec-2015 CMSDAS 2016, LPC Fermilab HBP\n",
    " - Updated: 10-Jan-2019 CMSDAS 2019, LPC Fermilab JMD\n",
    " - Updated: 03-Jan-2022 CMSDAS 2019, LPC Fermilab DRY\n",
    " - Updated: 07-Jan-2024 CMSDAS 2024, LPC Fermilab HBP\n",
    "\n",
    "```\n",
    "   N        = 25\n",
    "   b_hat_zz =  6.8 +/- 0.3\n",
    "   b_hat_zx =  2.6 +/- 0.4\n",
    "   s_hat    = 17.3 +/- 1.3 (mH = 125 GeV)\n",
    "            = 19.6 +/- 1.3 (mH = 126 GeV)\n",
    "```\n",
    " which pertains to data in the range $121.5 \\le m_H \\le 130.5\\text{ GeV}$\n",
    " for 7 and 8 TeV data. The main backgrounds are $p p \\rightarrow ZZ \\rightarrow 4l$ and $p p \\rightarrow Z + X \\rightarrow 4l$, where $X$ is typically one or ore jets. **Question**: How can one get four charged leptons from a Z boson?\n",
    " \n",
    "\n",
    "## Introduction\n",
    "\n",
    "The most important task in any non-trivial statistical analysis is constructing a statistical model that accurately describes the complicated analysis pipeline that yielded the observations. It is, therefore, important to document accurately the steps you have taken to arrive at the observations as it is this documentation that you will use to construct an accurare statistical model. \n",
    "\n",
    "For the purpose of this exercise, we'll assume that the results were obtained from the following analysis pipeline.\n",
    "\n",
    "- Selection criteria (cuts) were applied to calibrated collider data to select final states comprising two pairs of same-flavor opposite sign leptons, which yielded $N$ events.\n",
    "- The same cuts were applied to simulated $H$, $ZZ$ and $Z + X$ events. (We assume that the simulators have been tuned to match the characteristics of the observations). This yielded the signal estimate $\\hat{s} \\pm \\delta s$ and the background estimates $\\hat{b}_{ZZ} \\pm \\delta b_{ZZ}$ and $\\hat{b}_{ZX} \\pm \\delta b_{ZX}$. \n",
    "\n",
    "It is important to note that the signal and background estimates are **conditionally independent**, meaning that for a *fixed* set of values of all the parameters upon which the simulated events depend, that is, the **nuisance parameters**, the simulated events are **statistically independent**. The nuisance parameters include: \n",
    "\n",
    "- the scaling of  event counts by the ratio of the observed to simulated integrated luminosities, \n",
    "- the generator tune parameters, \n",
    "- the parton distribution function parameters, \n",
    "- the detector modeling parameters, \n",
    "- the trigger correction parameters, \n",
    "- the event correction parameters, and \n",
    "- the reconstructed particle energy scale and resolution parameters. \n",
    "\n",
    "The simulated events are also statistically independent of the observed events. Consequently, once we have constructed the statistical models for the observation, the signal, and each of the backgrounds, the overall statistical model is just the product of the individual statistical models. \n",
    "\n",
    "In this exercise, we do not develop the statistical model further. Note, however, that if we were to vary all of the nuisance parameters listed above (which, in principle, we should do simulteneously) and rerun the analysis on the new sets of simulated events, we would arrive at different signal and background estimates. If this were repeated say 1000 times, we would obtain 1000 sets of signal and background estimates $\\{ (\\hat{s} \\pm \\delta s, \\hat{b}_{ZZ} \\pm \\delta b_{ZZ}, \\hat{b}_{ZX} \\pm \\delta b_{ZX}) \\}$ that collectively define a distribution over the estimates that would fully and correctly account for the complicated correlations induced by the nuisance parameters, including all non-Gaussian effects. The full statistical model would be a sum over the 1000 instances of the original statistical model with each instance created with a different set of signal and background estimates.  In practice, to reduce the computational burden of this procedure, we make simplifying assumptions, some of which you will learn about in the long exercises.\n",
    "\n",
    "### Statistical Model versus Likelihood\n",
    "\n",
    "The basic assumption of statistics is that for every problem amenable to statistical analysis, a **data generation mechanisms**  exists that generate data at random according to an underlying probability distribution $\\mathbb{G}$. We assume that $\\mathbb{G}$ can be modeled with a parametric **statistical model** (or probability model), $p(X | \\theta)$ with parameters $\\theta$, of *potential* observations $X$. When an observation, e.g.,  $X = N$, is inserted into the statistical model we obtain a function $p(N | \\theta)$ called the **likelihood function** or likelihood for short. Since the observations are known  *constants*, the likelihood is no longer a function of the data but is still a function of the model parameters $\\theta$. (The likelihood is no more a function of $N$ than a Gaussian is a function of $\\pi$!) Because the likelihood is a function of $\\theta$ only, sometimes one writes ${\\cal L}(\\theta) = p(N | \\theta)$. However, it is better to use the notation $p(N | \\theta)$ to remind ourselves of the provenance of the likelihood.\n",
    "\n",
    "## Statistical Model\n",
    "\n",
    "We'll model *potential* event counts $X$ with a $\\text{Poisson}(X; n) = \\exp(-a) a^X \\, / \\, X!$ distribution, where the parameter $n$ is the mean count. In this version of **exercise_1**, we'll assume that each estimate of a background $\\hat{b}$ is the result of scaling a count $Q$ by a scale factor $q$, that is, we assume that\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{b} & = \\frac{Q}{q}, \\\\\n",
    "    \\delta b & = \\frac{\\sqrt{Q}}{q},\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where the estimated uncertainty $\\delta b$ follows from the fact that the standard deviation of a Poisson distribution with mean $n$ is $\\sqrt{n}$. We are given neither $Q$ nor $q$,  but we can estimate them using\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    Q & = \\left( \\frac{\\hat{b}}{\\delta b} \\right)^2, \\\\\n",
    "    q & = \\frac{\\hat{b}}{\\delta b^2} .\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Defining the quantities,\n",
    "\n",
    "\\begin{align}\n",
    "        B_j & = (\\hat{b}_j \\, / \\, \\delta b_j)^2, \\quad j = ZZ, ZX, \\\\\n",
    "    \\tau_j  & = \\hat{b}_j\\, / \\, \\delta b_j^2, \\\\\n",
    "        S & = (\\hat{s} \\, / \\, \\delta s)^2, \\\\\n",
    "        \\tau_s & = \\hat{s} \\, / \\, \\delta s^2,\n",
    "\\end{align}\n",
    "\n",
    "we can write the statistical model as follows,\n",
    "\n",
    "\\begin{align}\n",
    "    p(X, Y| \\mu, \\nu) & = \\text{Poisson}(X, \\mu  s + b_{ZZ} + b_{ZX}) \\nonumber\\\\\n",
    "                    & \\times \\text{Poisson}(B_{ZZ};  \\tau_{ZZ} b_{ZZ}) \\nonumber\\\\\n",
    "                    & \\times \\text{Poisson}(B_{ZX};  \\tau_{ZX} b_{ZX}) \\nonumber\\\\\n",
    "                    & \\times \\text{Poisson}(S;  \\tau_{s} s) .\n",
    "\\end{align}\n",
    "\n",
    "where we have split the model parameters $\\theta$ into the **parameter of interest** $\\mu$, the **signal strength**, and the nuisance parameters $\\nu = s, b_{ZZ}, b_{ZX}$ and written $S$, $B$, and the $\\tau$s as $Y$. The quantities $Y$ are often referred to as **auxiliary data** to distinguish them from the observations $X$. Note that if the Standard Model (SM) prediction is correct then we expect $\\mu = 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.28/06\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def createWorkspace(wsname, wsfilename):\n",
    "    # The most convenient way to use RooFit/RooStats is to \n",
    "    # make a workspace so that we can use its factory method\n",
    "    wspace = ROOT.RooWorkspace(wsname)\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Create parameters\n",
    "    #\n",
    "    # Use the factory method of the RooWorkspace to create\n",
    "    # parameters\n",
    "    #\n",
    "    # syntax:\n",
    "    #        <name>[value, min-value, max-value]\n",
    "    #-----------------------------------------------------\n",
    "    # observations\n",
    "    params = [('N',       25,     0,  50),\n",
    "    # ZZ background estimate        \n",
    "              ('b_hat_zz', 6.8,   0,  15),\n",
    "              ('db_zz',    0.3,   0,   5),\n",
    "    # Z+X background estimate\n",
    "              ('b_hat_zx', 2.6,   0,  15),\n",
    "              ('db_zx',    0.3,   0,   5),\n",
    "    # signal estimate (mH=125 GeV)       \n",
    "              ('s_hat',   17.3,   0,  25),\n",
    "              ('ds',       1.3,   0,   5),\n",
    "    # nuisance parameters\n",
    "              ('b_zx',    2.6,    0,  10),\n",
    "              ('b_zz',    6.8,    0,  15),\n",
    "              ('s',      17.3,    0,  25),\n",
    "    # parameter of interest\n",
    "              ('mu',      1.0,    0,   4)]\n",
    "\n",
    "    for t in params:\n",
    "        cmd = '{}[{}, {}, {}]'.format(t[0], t[1], t[2], t[3]) # Could also do .format(*t)?\n",
    "        wspace.factory(cmd)        \n",
    "    wspace.var('mu').SetTitle('#mu')\n",
    "\n",
    "    # fix all background and signal parameters\n",
    "    for t in params[1:-4]:\n",
    "        name = t[0]\n",
    "        print('=> make {:8s} = {:5.1f} constant'.format(name, wspace.var(name).getVal()))\n",
    "        wspace.var(name).setConstant()\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Create expressions\n",
    "    #\n",
    "    # syntax:\n",
    "    #        expr::<name>(\"expression\", var1, var2, ...)\n",
    "    #-----------------------------------------------------\n",
    "    exprs = ['B_zz(\"(b_hat_zz/db_zz)^2\", b_hat_zz, db_zz)',\n",
    "             'tau_zz(\"b_hat_zz/db_zz^2\", b_hat_zz, db_zz)',\n",
    "               \n",
    "             'B_zx(\"(b_hat_zx/db_zx)^2\", b_hat_zx, db_zx)',\n",
    "             'tau_zx(\"b_hat_zx/db_zx^2\", b_hat_zx, db_zx)',\n",
    "                              \n",
    "             'S(\"(s_hat/ds)^2\",   s_hat, ds)',\n",
    "             'tau_s(\"s_hat/ds^2\", s_hat, ds)',\n",
    "\n",
    "             'tau_zzb_zz(\"tau_zz*b_zz\", tau_zz, b_zz)',\n",
    "             'tau_zxb_zx(\"tau_zx*b_zx\", tau_zx, b_zx)',\n",
    "             'tau_ss(\"tau_s*s\", tau_s, s)',\n",
    "             \n",
    "             'n(\"mu*s + b_zz + b_zx\", mu, s, b_zz, b_zx)']\n",
    "        \n",
    "    for expr in exprs:\n",
    "        cmd = 'expr::{}'.format(expr)\n",
    "        wspace.factory(cmd)\n",
    "\n",
    "    print('\\neffective counts and scale factors')\n",
    "    print('B_zz = {:8.2f}, tau_zz = {:8.2f}'.format(wspace.function('B_zz').getVal(),\n",
    "                                           wspace.function('tau_zz').getVal()))\n",
    "\n",
    "    print('B_zx = {:8.2f}, tau_zx = {:8.2f}'.format(wspace.function('B_zx').getVal(),\n",
    "                                           wspace.function('tau_zx').getVal()))\n",
    "\n",
    "    print('S    = {:8.2f}, tau_s  = {:8.2f}'.format(wspace.function('S').getVal(),\n",
    "                                           wspace.function('tau_s').getVal()))\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Create pdfs\n",
    "    #\n",
    "    # syntax:\n",
    "    #        pdf_name::<name>(var1, var2, ...)\n",
    "    #\n",
    "    # where the \"Roo\" prefix is dropped in pdf_name, e.g.\n",
    "    #-----------------------------------------------------\n",
    "    pdfs = [('Poisson', 'pN',    '(N, n)'), \n",
    "            # scaled Poisson constraints (allowing non-integer B_zz, B_zx, and S)\n",
    "            ('Poisson', 'pB_zz', '(B_zz, tau_zzb_zz, 1)'), \n",
    "            ('Poisson', 'pB_zx', '(B_zx, tau_zxb_zx, 1)'),\n",
    "            ('Poisson', 'pS',    '(S,    tau_ss,     1)'),\n",
    "           ]\n",
    "    \n",
    "    prodpdf = ''\n",
    "    for pdfargs in pdfs:\n",
    "        wspace.factory('{}::{}{}'.format(pdfargs[0], pdfargs[1], pdfargs[2]))\n",
    "        name = pdfargs[1]\n",
    "        prodpdf += \"{}, \".format(name)\n",
    "    prodpdf = prodpdf[:-2] # remove last \", \"\n",
    "    \n",
    "    # multiply the pdfs together. use upper case PROD to\n",
    "    # do this\n",
    "    wspace.factory('PROD::model({})'.format(prodpdf))\n",
    "\n",
    "    # create a prior, since one is needed in Bayesian\n",
    "    # calculations\n",
    "    wspace.factory('Uniform::prior({mu, s, b_zz, b_zx})')\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    # Define a few useful sets. Now we need to decide\n",
    "    # whether or not to include B and S in the set obs of\n",
    "    # observations! \n",
    "    #-----------------------------------------------------\n",
    "    sets = [('obs',  'N'),           # observations\n",
    "            ('poi',  'mu'),          # parameter of interest\n",
    "            ('nuis', 's,b_zz,b_zx')] # nuisance parameters (leave no spaces)\n",
    "    for t in sets:\n",
    "        name, parlist = t\n",
    "        wspace.defineSet(name, parlist)\n",
    "    \n",
    "    #-----------------------------------------------------        \n",
    "    # create a dataset\n",
    "    #-----------------------------------------------------    \n",
    "    data = ROOT.RooDataSet('data', 'data', wspace.set('obs'))\n",
    "    data.add(wspace.set('obs'))\n",
    "    \n",
    "    # import dataset into workspace\n",
    "    wspace.Import(data)\n",
    "        \n",
    "    #-----------------------------------------------------\n",
    "    # Create model configuration. This is needed for the\n",
    "    # statistical analyses\n",
    "    #-----------------------------------------------------\n",
    "    cfg = ROOT.RooStats.ModelConfig('cfg')\n",
    "    cfg.SetWorkspace(wspace)\n",
    "    cfg.SetPdf(wspace.pdf('model'))\n",
    "    cfg.SetPriorPdf(wspace.pdf('prior'))\n",
    "    cfg.SetParametersOfInterest(wspace.set('poi'))\n",
    "    cfg.SetNuisanceParameters(wspace.set('nuis'))\n",
    "\n",
    "    # import model configuration into workspace\n",
    "    wspace.Import(cfg)\n",
    "\n",
    "    wspace.Print()\n",
    "    \n",
    "    # write out workspace\n",
    "    wspace.writeToFile(wsfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createWorkspace('CMSDAS', 'single_count_1.1.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def analyzeWorkspace(wsname, wsfilename):\n",
    "\n",
    "    # Open workspace file\n",
    "    wsfile = ROOT.TFile.Open(wsfilename)\n",
    "\n",
    "    # Get workspace\n",
    "    wspace = wsfile.Get(wsname) \n",
    "\n",
    "    # Get data\n",
    "    data = wspace.data('data')\n",
    "\n",
    "    # Get model configuration    \n",
    "    cfg  = wspace.obj('cfg')\n",
    "\n",
    "    #-----------------------------------------------------    \n",
    "    # Fit model to data\n",
    "    #-----------------------------------------------------\n",
    "    results = wspace.pdf('model').fitTo(data, ROOT.RooFit.Save())\n",
    "    results.Print()\n",
    "    \n",
    "    #-----------------------------------------------------    \n",
    "    # Compute interval based on profile likelihood\n",
    "    #-----------------------------------------------------\n",
    "    # suppress some (apparently) innocuous warnings\n",
    "    msgservice = ROOT.RooMsgService.instance()\n",
    "    msgservice.setGlobalKillBelow(ROOT.RooFit.FATAL)\n",
    "        \n",
    "    print('compute interval using profile likelihood')\n",
    "    plc = ROOT.RooStats.ProfileLikelihoodCalculator(data, cfg)\n",
    "    CL  = 0.683\n",
    "    plc.SetConfidenceLevel(CL)\n",
    "    plcInterval= plc.GetInterval()\n",
    "    lowerLimit = plcInterval.LowerLimit(wspace.var('mu'))\n",
    "    upperLimit = plcInterval.UpperLimit(wspace.var('mu'))\n",
    "\n",
    "    print('\\tPL {:4.1f}% CL interval = [{:5.2f}, {:5.2f}]'.format(100*CL, lowerLimit, upperLimit))\n",
    "\n",
    "    plcplot = ROOT.RooStats.LikelihoodIntervalPlot(plcInterval)      \n",
    "    plccanvas = ROOT.TCanvas('fig_PL_1.1', 'PL', 800, 400)\n",
    "    plccanvas.Divide(2, 1)\n",
    "    plccanvas.cd(1)\n",
    "    plcplot.SetRange(0,4)\n",
    "    plcplot.SetMaximum(3)\n",
    "    plcplot.Draw()\n",
    "    \n",
    "    # compute an 95% upper limit on mu by\n",
    "    # computing a 90% central interval and\n",
    "    # ignoring the lower limit\n",
    "    CL = 0.90\n",
    "    plc.SetConfidenceLevel(CL)\n",
    "    plcInterval = plc.GetInterval()\n",
    "    upperLimit = plcInterval.UpperLimit(wspace.var('mu'))\n",
    "\n",
    "    CL = 0.95\n",
    "    print('\\tPL {:4.1f}% upper limit = {:5.2f}\\n'.format(100*CL, upperLimit))\n",
    "      \n",
    "    plccanvas.cd(2)\n",
    "    plcplot2 = ROOT.RooStats.LikelihoodIntervalPlot(plcInterval)\n",
    "    plcplot2.SetRange(0,4)\n",
    "    plcplot2.SetMaximum(3)\n",
    "    plcplot2.Draw()\n",
    "    plccanvas.Update()\n",
    "    \n",
    "    #-----------------------------------------------------    \n",
    "    # Compute interval based on Bayesian calculator\n",
    "    #-----------------------------------------------------\n",
    "    print('compute interval using Bayesian calculator')\n",
    "    bc = ROOT.RooStats.BayesianCalculator(data, cfg)\n",
    "    CL  = 0.683\n",
    "    bc.SetConfidenceLevel(CL)\n",
    "    bcInterval = bc.GetInterval()\n",
    "    lowerLimit = bcInterval.LowerLimit()\n",
    "    upperLimit = bcInterval.UpperLimit()\n",
    "\n",
    "    print('\\tBayes {:4.1f}% CL interval = [{:5.2f}, {:5.2f}]'.format(100*CL, lowerLimit, upperLimit))\n",
    "\n",
    "    # calculate posterior density at 50 points\n",
    "    print(\"\\t\\t...be patient...!\")\n",
    "    bc.SetScanOfPosterior(50)\n",
    "    bcplot = bc.GetPosteriorPlot()\n",
    "    bccanvas = ROOT.TCanvas('fig_Bayes_1.1', 'Bayes', 800, 400)\n",
    "    bccanvas.Divide(2, 1)\n",
    "    bccanvas.cd(1)\n",
    "    bcplot.Draw()\n",
    "    bccanvas.Update()\n",
    "\n",
    "    # compute an 95% upper limit on mu\n",
    "    CL  = 0.950\n",
    "    bc.SetConfidenceLevel(CL)\n",
    "    # 0   = upper limit\n",
    "    # 0.5 = central limits (default)\n",
    "    # 1   = lower limit\n",
    "    bc.SetLeftSideTailFraction(0)\n",
    "    bcInterval = bc.GetInterval()\n",
    "    upperLimit = bcInterval.UpperLimit()\n",
    "\n",
    "    print('\\tBayes {:4.1f}% upper limit = {:5.2f}\\n'.format(100*CL, upperLimit))\n",
    "\n",
    "    # calculate posterior density at 50 points\n",
    "    bc.SetScanOfPosterior(50)\n",
    "    bcplot2 = bc.GetPosteriorPlot()\n",
    "    bccanvas.cd(2)\n",
    "    bcplot2.Draw()\n",
    "    bccanvas.Update()\n",
    "\n",
    "    # save canvases\n",
    "    plccanvas.Draw()\n",
    "    bccanvas.Draw()\n",
    "    plccanvas.SaveAs('.png')\n",
    "    bccanvas.SaveAs('.png')\n",
    "    return plccanvas, bccanvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plccanvas, bccanvas = analyzeWorkspace('CMSDAS', 'single_count_1.1.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
